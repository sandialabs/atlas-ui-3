{
  "data_sources": {
    "company-policies": {
      "name": "Company Policies",
      "description": "HR policies including remote work, expenses, PTO, and code of conduct",
      "compliance_level": "Internal",
      "required_groups": ["employee"],
      "documents": [
        {
          "id": "pol-001",
          "title": "Remote Work Policy",
          "content": "Remote Work Policy - Effective January 2024\n\n1. ELIGIBILITY\nAll full-time employees who have completed their 90-day probationary period are eligible for remote work arrangements. Contractors and temporary staff must obtain manager approval.\n\n2. WORK HOURS\nRemote employees must maintain core hours of 10:00 AM to 3:00 PM in their local timezone. Employees must be available for meetings and collaboration during these hours. Flexible scheduling outside core hours is permitted with manager approval.\n\n3. EQUIPMENT AND WORKSPACE\nThe company provides a laptop, monitor, and $500 home office stipend for ergonomic equipment. Employees are responsible for maintaining a dedicated workspace with reliable internet (minimum 50 Mbps). IT support is available 24/7 for technical issues.\n\n4. SECURITY REQUIREMENTS\nAll work must be performed on company-issued devices. VPN connection is mandatory when accessing internal systems. Employees must not work from public WiFi networks without VPN. Sensitive documents must not be printed at home.\n\n5. COMMUNICATION\nDaily check-ins via Slack are expected. Video must be enabled for all team meetings. Response time for messages should be within 2 hours during work hours. Weekly 1:1 meetings with managers are mandatory.\n\n6. PERFORMANCE EXPECTATIONS\nRemote employees are held to the same performance standards as in-office staff. Productivity is measured by output and deliverables, not hours logged. Quarterly reviews will assess remote work effectiveness.",
          "last_modified": "2024-01-15"
        },
        {
          "id": "pol-002",
          "title": "Expense Reimbursement Policy",
          "content": "Expense Reimbursement Policy\n\n1. ELIGIBLE EXPENSES\nTravel: Airfare (economy class), hotels ($200/night max), meals ($75/day), ground transportation.\nBusiness meals: Client entertainment up to $150/person with prior approval.\nOffice supplies: Up to $50/month without approval, larger purchases need manager sign-off.\nProfessional development: Conferences, courses, and certifications up to $2,500/year.\n\n2. SUBMISSION PROCESS\nAll expenses must be submitted within 30 days of incurrence via the Expensify app. Original receipts are required for expenses over $25. Manager approval is needed for expenses over $500. Finance reviews and processes approved expenses within 5 business days.\n\n3. TRAVEL BOOKING\nUse the corporate travel portal (Concur) for all travel bookings. Book flights at least 14 days in advance when possible. Preferred hotel chains: Marriott, Hilton, Hyatt. Rental cars require VP approval unless public transit is unavailable.\n\n4. NON-REIMBURSABLE EXPENSES\nPersonal entertainment, alcohol (except client entertainment), gym memberships, commuting costs, personal phone bills, airline upgrades, hotel minibar charges.\n\n5. CORPORATE CARDS\nDirectors and above receive corporate American Express cards. Monthly statements must be reconciled within 10 days. Misuse of corporate cards may result in revocation and disciplinary action.",
          "last_modified": "2024-02-01"
        },
        {
          "id": "pol-003",
          "title": "Code of Conduct",
          "content": "Employee Code of Conduct\n\nCORE VALUES\nIntegrity: We act honestly and ethically in all business dealings.\nRespect: We treat colleagues, customers, and partners with dignity.\nExcellence: We strive for quality in everything we do.\nInnovation: We embrace new ideas and continuous improvement.\n\nWORKPLACE BEHAVIOR\nHarassment and discrimination of any kind are strictly prohibited. This includes comments or actions based on race, gender, religion, age, disability, or sexual orientation. Report concerns to HR or use the anonymous ethics hotline.\n\nCONFLICTS OF INTEREST\nEmployees must disclose any personal or financial interests that could conflict with company interests. Outside employment requires written approval. Gifts from vendors over $100 must be reported and may need to be declined.\n\nCONFIDENTIALITY\nProprietary information must not be shared outside the company. NDAs must be signed before accessing sensitive projects. Customer data is subject to strict privacy regulations. Violations may result in termination and legal action.\n\nDATA PROTECTION\nHandle customer and employee data according to GDPR and CCPA requirements. Use strong passwords and enable two-factor authentication. Report any suspected data breaches immediately to the security team. Do not store sensitive data on personal devices.\n\nSOCIAL MEDIA\nPersonal social media use should not reflect negatively on the company. Do not share confidential information online. Official company statements are made only by authorized spokespersons.\n\nREPORTING VIOLATIONS\nUse the ethics hotline: 1-800-555-ETHICS or ethics@company.com. Reports can be made anonymously. Retaliation against reporters is strictly prohibited and grounds for termination.",
          "last_modified": "2024-01-01"
        },
        {
          "id": "pol-004",
          "title": "PTO and Leave Policy",
          "content": "Paid Time Off and Leave Policy\n\nANNUAL PTO ALLOCATION\nYears 0-2: 15 days PTO\nYears 3-5: 20 days PTO\nYears 6+: 25 days PTO\n\nPTO does not roll over to the next year. Unused PTO is not paid out except where required by state law. PTO requests should be submitted at least 2 weeks in advance for periods over 3 days.\n\nSICK LEAVE\nEmployees receive 10 sick days per year. Sick leave can be used for personal illness, medical appointments, or caring for immediate family. A doctor's note is required for absences exceeding 3 consecutive days.\n\nPARENTAL LEAVE\nPrimary caregivers: 16 weeks paid leave\nSecondary caregivers: 6 weeks paid leave\nLeave must be taken within 12 months of birth or adoption. Employees may request flexible return arrangements.\n\nBEREAVEMENT\nImmediate family (spouse, parent, child, sibling): 5 days\nExtended family (grandparent, in-law): 3 days\nClose friend: 1 day with manager approval\n\nHOLIDAYS\nThe company observes 10 federal holidays plus 2 floating holidays. Floating holidays must be used within the calendar year. Holiday schedule is published annually in December.\n\nJURY DUTY\nFull pay is provided for jury duty. Provide summons to HR. Return to work on days when court is not in session.",
          "last_modified": "2024-01-10"
        }
      ]
    },
    "technical-docs": {
      "name": "Technical Documentation",
      "description": "Engineering docs covering API auth, database schema, deployment, and architecture",
      "compliance_level": "Internal",
      "required_groups": ["engineering", "devops"],
      "documents": [
        {
          "id": "tech-001",
          "title": "API Authentication Guide",
          "content": "API Authentication Guide\n\nOVERVIEW\nOur API uses OAuth 2.0 with JWT tokens for authentication. All API requests must include a valid access token in the Authorization header.\n\nOBTAINING ACCESS TOKENS\n1. Register your application in the Developer Portal to get client_id and client_secret\n2. Exchange credentials for an access token:\n   POST /oauth/token\n   Content-Type: application/x-www-form-urlencoded\n\n   grant_type=client_credentials\n   client_id=your_client_id\n   client_secret=your_client_secret\n\n3. The response includes:\n   {\n     \"access_token\": \"eyJhbGciOiJSUzI1NiIs...\",\n     \"token_type\": \"Bearer\",\n     \"expires_in\": 3600\n   }\n\nUSING ACCESS TOKENS\nInclude the token in all API requests:\nAuthorization: Bearer eyJhbGciOiJSUzI1NiIs...\n\nTokens expire after 1 hour. Implement token refresh before expiration to avoid service interruption.\n\nTOKEN REFRESH\nPOST /oauth/token\ngrant_type=refresh_token\nrefresh_token=your_refresh_token\n\nSCOPES\nread:users - Read user profiles\nwrite:users - Modify user profiles\nread:data - Read application data\nwrite:data - Write application data\nadmin - Full administrative access\n\nRequest only the scopes your application needs. Excessive scope requests will be rejected.\n\nERROR HANDLING\n401 Unauthorized: Token is invalid or expired\n403 Forbidden: Token lacks required scope\n429 Too Many Requests: Rate limit exceeded (100 requests/minute)\n\nSECURITY BEST PRACTICES\nStore tokens securely, never in client-side code\nUse HTTPS for all API calls\nRotate client secrets every 90 days\nImplement token revocation for compromised credentials",
          "last_modified": "2024-03-01"
        },
        {
          "id": "tech-002",
          "title": "Database Schema Documentation",
          "content": "Database Schema Documentation\n\nUSERS TABLE\nPrimary table for user accounts.\n\nCREATE TABLE users (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    email VARCHAR(255) UNIQUE NOT NULL,\n    password_hash VARCHAR(255) NOT NULL,\n    first_name VARCHAR(100),\n    last_name VARCHAR(100),\n    role ENUM('admin', 'user', 'viewer') DEFAULT 'user',\n    status ENUM('active', 'inactive', 'suspended') DEFAULT 'active',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    last_login TIMESTAMP,\n    mfa_enabled BOOLEAN DEFAULT FALSE\n);\n\nIndexes:\n- idx_users_email ON users(email)\n- idx_users_status ON users(status)\n- idx_users_created ON users(created_at)\n\nPROJECTS TABLE\nStores project information.\n\nCREATE TABLE projects (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    owner_id UUID REFERENCES users(id),\n    status ENUM('active', 'archived', 'deleted') DEFAULT 'active',\n    visibility ENUM('public', 'private', 'team') DEFAULT 'private',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nAUDIT_LOGS TABLE\nTracks all system changes for compliance.\n\nCREATE TABLE audit_logs (\n    id BIGSERIAL PRIMARY KEY,\n    user_id UUID REFERENCES users(id),\n    action VARCHAR(50) NOT NULL,\n    resource_type VARCHAR(50) NOT NULL,\n    resource_id UUID,\n    old_values JSONB,\n    new_values JSONB,\n    ip_address INET,\n    user_agent TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nPartition by month for performance:\nCREATE TABLE audit_logs_2024_01 PARTITION OF audit_logs\nFOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\nPERFORMANCE NOTES\n- Use connection pooling (PgBouncer recommended, max 100 connections)\n- Enable query logging for slow queries (> 100ms)\n- Run VACUUM ANALYZE weekly on large tables\n- Archive audit_logs older than 2 years to cold storage",
          "last_modified": "2024-02-15"
        },
        {
          "id": "tech-003",
          "title": "Deployment Pipeline",
          "content": "Deployment Pipeline Documentation\n\nENVIRONMENTS\n1. Development (dev) - For active development and testing\n2. Staging (stg) - Pre-production validation\n3. Production (prod) - Live customer-facing environment\n\nPIPELINE STAGES\n\nStage 1: Build\n- Triggered on push to main branch or PR creation\n- Runs linting (ESLint, Pylint) and code formatting checks\n- Builds Docker images with commit SHA tags\n- Runs unit tests (minimum 80% coverage required)\n- Publishes images to private registry (gcr.io/company-prod)\n\nStage 2: Security Scan\n- Container scanning with Trivy for vulnerabilities\n- SAST with SonarQube (no critical/high issues allowed)\n- Dependency check for known CVEs\n- Secrets scanning to prevent credential leaks\n\nStage 3: Deploy to Staging\n- Automatic deployment after build success\n- Blue-green deployment strategy\n- Runs integration tests against staging APIs\n- Performance tests (response time < 200ms at P95)\n- Smoke tests for critical user journeys\n\nStage 4: Production Deployment\n- Requires manual approval from tech lead\n- Canary deployment: 10% traffic for 30 minutes\n- Automatic rollback if error rate exceeds 1%\n- Full rollout after canary validation\n- Post-deployment verification tests\n\nROLLBACK PROCEDURES\nAutomatic rollback triggers:\n- Error rate > 1% for 5 minutes\n- P95 latency > 500ms\n- Health check failures\n\nManual rollback:\nkubectl rollout undo deployment/app-name -n production\n\nMONITORING\n- Datadog for metrics and APM\n- PagerDuty for alerting\n- Deployment notifications in #deployments Slack channel\n\nHOTFIX PROCESS\n1. Create hotfix branch from production tag\n2. Apply minimal fix with tests\n3. Fast-track approval (on-call engineer + manager)\n4. Direct deploy to production, backport to main",
          "last_modified": "2024-03-10"
        },
        {
          "id": "tech-004",
          "title": "Microservices Architecture",
          "content": "Microservices Architecture Overview\n\nSERVICE CATALOG\n\n1. API Gateway (api-gateway)\n   - Routes external requests to internal services\n   - Handles authentication and rate limiting\n   - Technology: Kong on Kubernetes\n   - Port: 443 (external), 8000 (internal)\n\n2. User Service (user-service)\n   - Manages user accounts, profiles, and authentication\n   - Owns the users database\n   - Technology: Python/FastAPI\n   - Dependencies: PostgreSQL, Redis\n\n3. Project Service (project-service)\n   - Handles project CRUD operations\n   - Manages project memberships and permissions\n   - Technology: Node.js/Express\n   - Dependencies: PostgreSQL, Elasticsearch\n\n4. Notification Service (notification-service)\n   - Sends emails, SMS, and push notifications\n   - Queue-based for reliability\n   - Technology: Go\n   - Dependencies: RabbitMQ, SendGrid, Twilio\n\n5. Analytics Service (analytics-service)\n   - Collects and processes usage metrics\n   - Generates reports and dashboards\n   - Technology: Python/Flask\n   - Dependencies: ClickHouse, Kafka\n\nCOMMUNICATION PATTERNS\n- Synchronous: REST APIs for client-facing operations\n- Asynchronous: RabbitMQ for background jobs\n- Event streaming: Kafka for real-time data pipeline\n\nSERVICE MESH\nUsing Istio for:\n- Mutual TLS between services\n- Traffic management and load balancing\n- Observability and distributed tracing\n- Circuit breaking for fault tolerance\n\nSCALING POLICIES\n- Horizontal Pod Autoscaler based on CPU (target 70%)\n- Custom metrics autoscaling for queue depth\n- Minimum replicas: 2 (prod), 1 (staging)\n- Maximum replicas: 20 (prod), 5 (staging)\n\nHEALTH CHECKS\nAll services must implement:\nGET /health - Basic liveness check\nGET /ready - Readiness with dependency checks\nResponse time must be < 100ms for health endpoints",
          "last_modified": "2024-02-28"
        }
      ]
    },
    "product-knowledge": {
      "name": "Product Knowledge Base",
      "description": "Public product docs with getting started, troubleshooting, features, and API reference",
      "compliance_level": "Public",
      "required_groups": [],
      "documents": [
        {
          "id": "prod-001",
          "title": "Getting Started Guide",
          "content": "Getting Started with DataFlow Pro\n\nWELCOME\nDataFlow Pro is an enterprise data integration platform that connects your applications, databases, and cloud services. This guide will help you set up your first data pipeline in under 30 minutes.\n\nSYSTEM REQUIREMENTS\n- Modern web browser (Chrome, Firefox, Safari, Edge)\n- Network access to source and destination systems\n- API credentials for connected services\n\nSTEP 1: CREATE YOUR ACCOUNT\nVisit app.dataflowpro.com and click \"Start Free Trial\". Enter your business email and create a password. Verify your email within 24 hours to activate your account.\n\nSTEP 2: CONNECT YOUR FIRST SOURCE\nNavigate to Connections > Add New. Select your data source type (Database, API, File Storage, or SaaS App). Enter the connection details and credentials. Click \"Test Connection\" to verify access.\n\nSupported sources include:\n- Databases: PostgreSQL, MySQL, SQL Server, Oracle, MongoDB\n- Cloud Storage: AWS S3, Google Cloud Storage, Azure Blob\n- SaaS Apps: Salesforce, HubSpot, Zendesk, Shopify\n- APIs: Any REST or GraphQL endpoint\n\nSTEP 3: CREATE A PIPELINE\nClick \"New Pipeline\" from the dashboard. Select your source and destination connections. Choose sync frequency: real-time, hourly, daily, or custom. Map fields between source and destination. Enable the pipeline to start syncing.\n\nSTEP 4: MONITOR YOUR DATA\nView sync status on the Pipeline Dashboard. Check data quality metrics and error logs. Set up alerts for sync failures or data anomalies.\n\nNEED HELP?\nDocumentation: docs.dataflowpro.com\nSupport: support@dataflowpro.com\nCommunity: community.dataflowpro.com",
          "last_modified": "2024-03-15"
        },
        {
          "id": "prod-002",
          "title": "Troubleshooting Common Issues",
          "content": "Troubleshooting Guide\n\nCONNECTION ERRORS\n\nProblem: \"Connection refused\" error\nCauses:\n- Firewall blocking outbound connections\n- Incorrect hostname or port\n- Service not running on destination\n\nSolutions:\n1. Whitelist DataFlow Pro IPs: 52.1.2.3, 52.1.2.4, 52.1.2.5\n2. Verify hostname resolves correctly: nslookup hostname\n3. Check service status on destination server\n4. Try connecting from a different network\n\nProblem: \"Authentication failed\" error\nCauses:\n- Invalid credentials\n- Expired API key or token\n- Insufficient permissions\n\nSolutions:\n1. Regenerate API credentials in the source system\n2. Verify the user has read access to required tables\n3. Check for password special characters (escape if needed)\n4. Ensure OAuth token hasn't expired\n\nSYNC ISSUES\n\nProblem: Sync is slow or timing out\nCauses:\n- Large data volume without pagination\n- Network latency\n- Source system under heavy load\n\nSolutions:\n1. Enable incremental sync instead of full refresh\n2. Add filters to reduce data volume\n3. Schedule syncs during off-peak hours\n4. Increase timeout in pipeline settings\n\nProblem: Missing or duplicate data\nCauses:\n- Primary key not configured correctly\n- Sync interrupted mid-process\n- Schema changes in source\n\nSolutions:\n1. Verify primary key column is set in pipeline config\n2. Enable \"upsert\" mode for idempotent syncs\n3. Run a full refresh to resync all data\n4. Check field mappings after schema changes\n\nPERFORMANCE OPTIMIZATION\n\nFor large datasets (>1M rows):\n- Use change data capture (CDC) when available\n- Partition data by date or region\n- Enable parallel processing (up to 4 threads)\n- Consider staging to intermediate storage\n\nContact support@dataflowpro.com for enterprise performance tuning.",
          "last_modified": "2024-03-12"
        },
        {
          "id": "prod-003",
          "title": "Feature Comparison by Plan",
          "content": "DataFlow Pro Plans and Features\n\nSTARTER PLAN - $49/month\nBest for small teams and simple integrations\n\nIncluded:\n- 5 active pipelines\n- 100,000 rows synced per month\n- 10 pre-built connectors\n- Daily sync frequency\n- Email support (48-hour response)\n- 7-day data retention\n\nLimitations:\n- No custom connectors\n- No real-time sync\n- Single user only\n\nPROFESSIONAL PLAN - $199/month\nBest for growing teams with complex data needs\n\nEverything in Starter, plus:\n- 25 active pipelines\n- 1,000,000 rows synced per month\n- 50+ pre-built connectors\n- Hourly sync frequency\n- Priority email support (24-hour response)\n- 30-day data retention\n- Up to 5 team members\n- Custom field transformations\n- Slack notifications\n- API access\n\nENTERPRISE PLAN - Custom pricing\nBest for large organizations with advanced requirements\n\nEverything in Professional, plus:\n- Unlimited pipelines\n- Unlimited row syncs\n- All connectors including custom\n- Real-time sync (CDC)\n- Dedicated support engineer\n- 1-year data retention\n- Unlimited team members\n- SSO/SAML integration\n- Custom SLA (99.9% uptime)\n- On-premise deployment option\n- SOC 2 Type II compliance\n- HIPAA compliance (healthcare)\n- Dedicated infrastructure\n\nADD-ONS (Available on any plan)\n- Additional connectors: $25/month each\n- Extended retention: $50/month per year\n- Priority support upgrade: $100/month\n- Custom connector development: Starting at $2,500\n\nContact sales@dataflowpro.com for Enterprise pricing.",
          "last_modified": "2024-03-01"
        },
        {
          "id": "prod-004",
          "title": "API Reference Overview",
          "content": "DataFlow Pro API Reference\n\nBASE URL\nhttps://api.dataflowpro.com/v1\n\nAUTHENTICATION\nAll API requests require a Bearer token in the Authorization header.\nGenerate API keys in Settings > API Keys.\n\nAuthorization: Bearer dfp_live_abc123xyz\n\nRATE LIMITS\n- Standard plans: 100 requests/minute\n- Enterprise plans: 1000 requests/minute\n- Burst allowance: 2x limit for 10 seconds\n\nENDPOINTS\n\nGET /pipelines\nList all pipelines in your account.\nQuery params: status, limit, offset\n\nPOST /pipelines\nCreate a new pipeline.\nRequired fields: name, source_id, destination_id, schedule\n\nGET /pipelines/{id}\nGet pipeline details including configuration and stats.\n\nPUT /pipelines/{id}\nUpdate pipeline configuration.\n\nDELETE /pipelines/{id}\nDelete a pipeline. This action cannot be undone.\n\nPOST /pipelines/{id}/run\nTrigger an immediate sync for the pipeline.\n\nGET /pipelines/{id}/runs\nList recent sync runs with status and metrics.\n\nGET /connections\nList all configured connections.\n\nPOST /connections\nCreate a new connection.\nRequired fields: name, type, credentials\n\nPOST /connections/{id}/test\nTest connection connectivity.\n\nWEBHOOKS\nConfigure webhooks to receive real-time notifications:\n- pipeline.sync.started\n- pipeline.sync.completed\n- pipeline.sync.failed\n- pipeline.error.threshold\n\nWebhook payloads are signed with HMAC-SHA256.\nVerify using the X-DataFlow-Signature header.\n\nSDK LIBRARIES\nPython: pip install dataflowpro\nNode.js: npm install @dataflowpro/sdk\nRuby: gem install dataflowpro\nGo: go get github.com/dataflowpro/go-sdk\n\nFull API documentation: docs.dataflowpro.com/api",
          "last_modified": "2024-03-08"
        }
      ]
    }
  },
  "users_groups": {
    "alice@example.com": ["employee", "engineering"],
    "bob@example.com": ["employee", "sales"],
    "charlie@example.com": ["employee", "engineering", "devops"],
    "test@test.com": ["employee", "engineering", "devops", "admin"],
    "guest@example.com": []
  }
}
